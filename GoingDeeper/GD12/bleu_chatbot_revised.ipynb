{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedeb042",
   "metadata": {},
   "source": [
    "# Chatbot with BLUE score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27653701",
   "metadata": {},
   "source": [
    "__결과__\n",
    "  - 학습이 수렴하지 않아 대답을 생성하지 못한다.\n",
    "  - ~~이유는 아직 파악이 안 됨~~ loss가 nan으로 연산되는 오류; 모자란 vocab size  \n",
    "    범위를 벗어난 인덱스의 토큰 임베딩은 원소가 모두 0인 벡터로 맵핑된다.\n",
    "  - nan 오류를 해결 후엔 잘 수렴한다.\n",
    "  - 훈련셋에 있는 질문엔 비교적 잘 답변한다. 하지만 새로운 질문을 입력하면 답변이 좋지 않다.\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "65315216",
   "metadata": {},
   "source": [
    "# !pip install --upgrade gensim==3.8.3 ### 노션 참고\n",
    "\n",
    "Collecting gensim==3.8.3\n",
    "  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n",
    "     |████████████████████████████████| 23.4 MB 4.5 MB/s            \n",
    "  Preparing metadata (setup.py) ... done\n",
    "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
    "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
    "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
    "Requirement already satisfied: smart_open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
    "Building wheels for collected packages: gensim\n",
    "  Building wheel for gensim (setup.py) ... done\n",
    "  Created wheel for gensim: filename=gensim-3.8.3-cp39-cp39-linux_x86_64.whl size=24328218 sha256=c49ddb213cd83b9bb8014fb7218d261b11485a7c8ca967b22d591c9713669180\n",
    "  Stored in directory: /aiffel/.cache/pip/wheels/ca/5d/af/618594ec2f28608c1d6ee7d2b7e95a3e9b06551e3b80a491d6\n",
    "Successfully built gensim\n",
    "Installing collected packages: gensim\n",
    "  Attempting uninstall: gensim\n",
    "    Found existing installation: gensim 4.1.2\n",
    "    Uninstalling gensim-4.1.2:\n",
    "      Successfully uninstalled gensim-4.1.2\n",
    "Successfully installed gensim-3.8.3\n",
    "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eda6b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n",
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gensim\n",
    "# from gensim.models import KeyedVectors\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6ae74dd",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "os.system(\"mkdir -p ~/aiffel/gd_12/data/\")\n",
    "os.system(\"ln -s ~/data/* ~/aiffel/gd_12/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3bb29a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_path = \"/aiffel/aiffel/gd_12/\"\n",
    "raw_data = pd.read_csv(d_path+\"ChatbotData.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c994e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # 모든 입력을 소문자로 변환; 알파벳, 문장부호, 한글만 남기고 모두 제거\n",
    "    sentence = re.sub(r\"[^0-9a-zA-Z가-힣ㄱ-ㅎㅏ-ㅣ\\?.!,]+\", \" \", sentence.lower())\n",
    "        \n",
    "    # 문장 앞뒤의 불필요한 공백을 제거합니다\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f35c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "questions = [str(s) for s in  raw_data[\"Q\"]]\n",
    "answers = [str(s) for s in  raw_data[\"A\"]]\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec164f1",
   "metadata": {},
   "source": [
    "__데이터 전처리 및 토큰화__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b05983",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Src, Tgt 한 쪽의 중복도 허용하지 않는 경우 예를들어,\n",
    "(\"A\", \"B\")\n",
    "(\"A\", \"D\")\n",
    "(\"C\", \"D\")\n",
    "위 경우 (\"A\", \"D\") 쌍이 먼저 포함되면\n",
    "(\"A\", \"B\")나 (\"C\", \"D\") 모두 사용할 수 없다.\n",
    "'''\n",
    "def build_corpus(src_raw, tgt_raw, max_len):\n",
    "    # 문장 전처리\n",
    "    src_corpus = [preprocess_sentence(s) for s in  src_raw]\n",
    "    tgt_corpus = [preprocess_sentence(s) for s in  tgt_raw]\n",
    "    \n",
    "    # 토큰화\n",
    "    src_corpus = [mecab_split(s) for s in  src_corpus]\n",
    "    tgt_corpus = [mecab_split(s) for s in  tgt_corpus]\n",
    "    \n",
    "    # 긴 문장 제외\n",
    "    filtered_idcs = [i for i, s in enumerate(zip(src_corpus, tgt_corpus))\n",
    "                             if len(s[0])<=max_len and len(s[1])<=max_len]\n",
    "    \n",
    "    # 중복문 제외\n",
    "    src_result, tgt_result = list(), list()\n",
    "    src_instnts, tgt_instnts = dict(), dict()\n",
    "    for i in filtered_idcs:\n",
    "        s = src_instnts.setdefault(src_corpus[i], []); s.append(i)\n",
    "        t = tgt_instnts.setdefault(tgt_corpus[i], []); t.append(i)\n",
    "        if len(t)>1: continue\n",
    "        if len(s)>1: continue\n",
    "        src_result.append(src_corpus[i])\n",
    "        tgt_result.append(tgt_corpus[i])\n",
    "    \n",
    "    return src_result, tgt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a339b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "\n",
    "def mecab_split(sentence):\n",
    "    return tuple(mecab.morphs(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba0d289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus , ans_corpus = build_corpus(questions, answers, max_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e54e885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 샘플 수 : 7646\n",
      "전처리 샘플 수 : 7646\n"
     ]
    }
   ],
   "source": [
    "print('전처리 샘플 수 :', len(que_corpus))\n",
    "print('전처리 샘플 수 :', len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd10d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv_path = \"/aiffel/aiffel/gd_12/\"\n",
    "model_dir = os.path.join(wv_path, 'ko.bin')\n",
    "\n",
    "# from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "# wv = Word2VecKeyedVectors.load(model_dir)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "ko_w2v = Word2Vec.load(model_dir)\n",
    "# model = Word2Vec.load_word2vec_format(model_dir, binary=True)\n",
    "\n",
    "# with open(model_dir, 'rb') as f:\n",
    "#     wv = Word2VecKeyedVectors.load_word2vec_format(\n",
    "#             f, binary=True, unicode_errors='ignore',) # , limit=500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "757bfc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('정직', 0.7132465839385986),\n",
       " ('겸손', 0.6951979398727417),\n",
       " ('완고', 0.6625800728797913),\n",
       " ('근면', 0.6611404418945312),\n",
       " ('친절', 0.6598717570304871),\n",
       " ('냉정', 0.6536935567855835),\n",
       " ('솔직', 0.6535051465034485),\n",
       " ('현명', 0.6468929052352905),\n",
       " ('충실', 0.6405693292617798),\n",
       " ('공평', 0.6393523216247559)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_w2v.wv.most_similar(\"성실\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83600082",
   "metadata": {},
   "source": [
    "__데이터 증강__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a41a0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(tokens, wv):\n",
    "    # tokens = sentence.split()\n",
    "\n",
    "    selected_tok = random.randrange(len(tokens))\n",
    "\n",
    "    result = []\n",
    "    for idx, tok in enumerate(tokens):\n",
    "        if idx is selected_tok:\n",
    "            try:\n",
    "                result.append(wv.most_similar(tok)[0][0])\n",
    "            # 로드된 word2vec 모델에 단어가 없을 경우\n",
    "            except KeyError as E:\n",
    "                return None\n",
    " \n",
    "        else:\n",
    "            result.append(tok)\n",
    "\n",
    "    return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe8b978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f97f8f678b54683953abe86ebd514f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_src_tgt = ([], [])\n",
    "random.seed(25)\n",
    "\n",
    "for old_src, old_tgt in tqdm(zip(que_corpus, ans_corpus)):\n",
    "    new_src = lexical_sub(old_src, ko_w2v.wv)\n",
    "    new_tgt = lexical_sub(old_tgt, ko_w2v.wv)\n",
    "    if new_src is not None: \n",
    "        new_src_tgt[0].append(new_src)\n",
    "        new_src_tgt[1].append(old_tgt)\n",
    "    if new_tgt is not None: \n",
    "        new_src_tgt[0].append(old_src)\n",
    "        new_src_tgt[1].append(new_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92aaa359",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13334 13334\n",
      "('12', '시', '땡', '캐치') ('하루', '가', '또', '가', '네요', '.')\n",
      "('12', '시', '땡', '!') ('일주일', '가', '또', '가', '네요', '.')\n",
      "('1', '중퇴', '학교', '떨어졌', '어') ('위로', '해', '드립니다', '.')\n",
      "('3', '박', '4', '일', '놀', '러', '가', '기에', '싶', '다') ('여행', '은', '언제나', '좋', '죠', '.')\n",
      "('3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다') ('항해', '은', '언제나', '좋', '죠', '.')\n",
      "('ppl', '강하', '네') ('눈살', '이', '찌푸려', '지', '죠', '.')\n",
      "('sns', '들어맞', '팔', '왜', '안', '하', '지', 'ㅠㅠ') ('잘', '모르', '고', '있', '을', '수', '도', '있', '어요', '.')\n",
      "('sns', '맞', '팔', '왜', '안', '하', '지', 'ㅠㅠ') ('잘', '모르', '고', '있', '을', '수', '도', '있', '어요', '는데')\n",
      "('sns', '분간', '낭비', '인', '거', '아', '는데', '매일', '하', '는', '중') ('시간', '을', '정하', '고', '해', '보', '세요', '.')\n",
      "('sns', '시간', '낭비', '인', '거', '아', '는데', '매일', '하', '는', '중') ('시간', '을', '정하', '기에', '해', '보', '세요', '.')\n",
      "('sns', '보', '면', '나', '만', '빼', '고', '다', '사랑', '해', '보여') ('자랑', '하', '는', '자리', '니까요', '.')\n",
      "('sns', '보', '면', '나', '만', '빼', '고', '다', '행복', '해', '보여') ('자랑', '하', '는', '자리', 'ㄴ데요', '.')\n",
      "('가끔', '궁금하', '해') ('그', '사람', '도', '그럴', '거', '예요', '.')\n",
      "('가끔', '궁금', '해') ('그', '사람', '도', '그럴', '거', '예요', '는데')\n",
      "('가끔', '은데', '혼자', '인', '게', '좋', '다') ('혼자', '를', '즐기', '세요', '.')\n",
      "('가끔', '은', '혼자', '인', '게', '좋', '다') ('혼자', '를', '즐기', '세요', '는데')\n",
      "('가난', '한', '자마자', '의', '설움') ('돈', '은', '다시', '들어올', '거', '예요', '.')\n",
      "('가난', '한', '자', '의', '설움') ('돈', '은', '다시', '들어올', '것', '예요', '.')\n",
      "('가만', '있', '어도', '오줌', '난다') ('땀', '을', '식혀', '주', '세요', '.')\n",
      "('가만', '있', '어도', '땀', '난다') ('땀', '을', '식혀', '주', 'ㅂ시오', '.')\n"
     ]
    }
   ],
   "source": [
    "print(len(new_src_tgt[0]), len(new_src_tgt[1]))\n",
    "\n",
    "for i in range(20):\n",
    "    print(new_src_tgt[0][i], new_src_tgt[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64e3c8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_len:  1\n",
      "max_len:  32\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = que_corpus + new_src_tgt[0]\n",
    "dec_corpus = ans_corpus + new_src_tgt[1]\n",
    "dec_corpus = [ [\"<bos>\"]+list(s)+[\"<eos>\"] for s in dec_corpus ]\n",
    "\n",
    "min_len = min(len(s) for s in enc_corpus+dec_corpus)\n",
    "max_len = max(len(s) for s in enc_corpus+dec_corpus)\n",
    "\n",
    "print(\"min_len: \", min_len)\n",
    "print(\"max_len: \", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d3e4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 3\n",
    "max_len = 30\n",
    "\n",
    "q_list, a_list = [], []\n",
    "\n",
    "for q, a in zip(enc_corpus, dec_corpus):\n",
    "    if len(q)<min_len: continue\n",
    "    if len(a)<min_len: continue   \n",
    "    if len(q)>max_len: continue\n",
    "    if len(a)>max_len: continue\n",
    "    q_list.append(q); a_list.append(a)\n",
    "\n",
    "enc_corpus, dec_corpus = q_list, a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34fcd5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20260 20260\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_corpus), len(dec_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f73edabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('12', '시', '땡', '!')\n"
     ]
    }
   ],
   "source": [
    "print(enc_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc72101",
   "metadata": {},
   "source": [
    "__훈련-검증 분리__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b00a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
    "                                enc_corpus, dec_corpus, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52a42168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('나', '를', '좋아하', '는', '애', '랑', '친구', '가능', '?')\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604eb25",
   "metadata": {},
   "source": [
    "__토크나이저 사전 생성__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73c50d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "                                filters='', lower=False, oov_token='')\n",
    "ko_tokenizer.fit_on_texts(list(s) for s in enc_train+dec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aab64478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 6951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '', '.', '<bos>', '<eos>', '이', '는', '하', '을', '가']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocab Size:\", VOCAB_SIZE:=(max(ko_tokenizer.index_word)+1))\n",
    "# mecab_tokenizer.sequences_to_texts([mecab_tensor[100]])\n",
    "# tensor = tokenizer.texts_to_sequences(corpus)\n",
    "ko_tokenizer.sequences_to_texts([[i] for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c27e3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_tokenizer.texts_to_sequences([[\"<unk>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a1b8df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_tokenizer.texts_to_sequences([[\"<uuuunk>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "457abc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_tokenizer.texts_to_sequences([[\"\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc453f9",
   "metadata": {},
   "source": [
    "__시퀀스 패딩__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f704aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "def padded_seqs(corpus, tokenizer):  # corpus: Tokenized Sentence's List\n",
    "    \n",
    "    if isinstance(corpus[0], list):\n",
    "        tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    else: \n",
    "        tensor = tokenizer.texts_to_sequences(list(s) for s in corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                                tensor, maxlen=MAX_LEN, padding='post')\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6851bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = padded_seqs(enc_train, ko_tokenizer)\n",
    "enc_val = padded_seqs(enc_val, ko_tokenizer)\n",
    "dec_train = padded_seqs(dec_train, ko_tokenizer)\n",
    "dec_val = padded_seqs(dec_val, ko_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88d98009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  22   32  170    6   94  134   45  211   18    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  96  560    2    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1251    9    6   24  574   20   18    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "[[   3  820   90   70   36   33   49    2    4    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   3 1343   41  369    9   14   64   22  171    2    4    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   3 5922    8 5923  259   19   12    2    4    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[:3])\n",
    "print(dec_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3224ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 # 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (enc_train, dec_train) ).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32a2e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return tf.cast(sinusoid_table , tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d859587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "    \n",
    "    dec_padd_mask = generate_padding_mask(tgt)\n",
    "    causality_mask = generate_causality_mask(tgt.shape[1])\n",
    "    combined_mask = tf.maximum(dec_padd_mask, causality_mask) ###\n",
    "\n",
    "    return enc_mask, combined_mask, dec_enc_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13c43ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        \n",
    "        QK = tf.matmul(Q, K, transpose_b=True) # LMS 예시\n",
    "        scaled_qk = QK / tf.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None: scaled_qk += (mask * -1e9)\n",
    "       \n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1) # 예시 참고\n",
    "        out = tf.matmul(attentions, V)\n",
    "        \n",
    "        return out, attentions\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "    \n",
    "        seq_len = x.shape[1]\n",
    "        split_x = tf.reshape(x,\n",
    "                         [-1, seq_len, self.num_heads, self.depth] )\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "  \n",
    "        seq_len = x.shape[2] # 예시 참고\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x,\n",
    "                         [-1, seq_len, self.d_model] )\n",
    "        \n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "  \n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_q(K)\n",
    "        WV = self.W_q(V)\n",
    "        \n",
    "        WQ_split = self.split_heads(WQ) # 예시 참고\n",
    "        WK_split = self.split_heads(WK)\n",
    "        WV_split = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "                                    WQ_split, WK_split, WV_split, mask )\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fa086ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aeba1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask, training):\n",
    "\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb1255e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecoderLayer 클래스를 작성하세요.\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x1, x2, c_mask, mask, training):\n",
    "\n",
    "        residual = x1\n",
    "        norm_x1 = self.norm_1(x1)\n",
    "        out, dec_attn = self.dec_self_attn(norm_x1, norm_x1, norm_x1, c_mask) ###\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    " \n",
    "        residual = out\n",
    "        norm_x1 = self.norm_2_1(out)\n",
    "        norm_x2 = self.norm_2_2(x2) \n",
    "        out, dec_enc_attn = self.enc_dec_attn(norm_x1, norm_x2, norm_x2, mask)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "        \n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63890874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask, training):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask, training)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f0e2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask, training):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "              self.dec_layers[i](out, enc_out, causality_mask, padding_mask, training)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3ecd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.2,\n",
    "                 shared=True,\n",
    "                 shared_emb=False,):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.shared = shared # 예시 참고\n",
    "        self.shared_emb = shared_emb # 예시 참고\n",
    "        \n",
    "        if shared_emb:\n",
    "            self.src_embedding = self.tgt_embedding = \\\n",
    "                        tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.src_embedding = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.tgt_embedding = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model) \n",
    "        \n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(tgt_vocab_size) # 예시 참고\n",
    "        \n",
    "        if shared : # 예시 참고\n",
    "            self.shared_weights = self.tgt_embedding.weights\n",
    "            self.linear.set_weights(tf.transpose(self.shared_weights))\n",
    "        \n",
    "        # self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def embedding(self, emb, x, shared=False):\n",
    "  \n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        if shared: out *= tf.math.sqrt(self.d_model)\n",
    "        pos_enc = self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = out + pos_enc\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, combined_mask, dec_enc_mask, training):\n",
    "        \n",
    "        if self.shared_emb: enc_shared = self.shared\n",
    "        else: enc_shared = False\n",
    "        \n",
    "        enc_in = self.embedding(self.src_embedding, enc_in, enc_shared)\n",
    "        dec_in = self.embedding(self.tgt_embedding, dec_in, self.shared)\n",
    "        \n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask, training)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(\n",
    "                            dec_in, enc_out, combined_mask, dec_enc_mask, training)\n",
    "        \n",
    "        logits = self.linear(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51a18ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # 실제 시퀀스중 0인 값을 0, 나머지 1로 반환\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # 실제 시퀀스중 0값을 갖는 위치의 손실에 0을 곱해줌\n",
    "    loss_ *= mask\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a994d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ad7170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, combined_mask, dec_enc_mask = generate_masks(src, tgt)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "                model(src, tgt, enc_mask, combined_mask, dec_enc_mask, training=True)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "\n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss, enc_attns, combined_mask, dec_enc_attns\n",
    "\n",
    "@tf.function  # 예제코드 재사용\n",
    "def eval_step(src, tgt, model):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, combined_mask, dec_enc_mask = generate_masks(src, tgt)\n",
    "    \n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            model(src, tgt, enc_mask, combined_mask, dec_enc_mask, training=False)\n",
    "    \n",
    "    loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d87ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (enc_train, dec_train) ).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "N_LAYERS = 6\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "D_FF = 2048\n",
    "pos_len = MAX_LEN # 40\n",
    "DROPOUT = 0.1\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=N_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=pos_len,\n",
    "    dropout=DROPOUT,\n",
    "    shared=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "515079f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(D_MODEL, warmup_steps=1000)\n",
    "optimizer = tf.keras.optimizers.Adam(#0.00001)\n",
    "                    learning_rate,beta_1=0.9,beta_2=0.98, epsilon=1e-9)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(0.00001)\n",
    "#                         learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4b68b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: (64, 1, 1, 30)\n",
      "dec_enc_mask: (64, 1, 1, 30)\n",
      "dec_mask: (64, 1, 30, 30)\n",
      "enc_attns: (64, 8, 30, 30)\n",
      "Decoder Output: (64, 30, 6951)\n",
      "dec_attns: (64, 8, 30, 30)\n",
      "dec_enc_attns: (64, 8, 30, 30)\n",
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1779456   \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            multiple                  7100928   \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  7896576   \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             multiple                  1786407   \n",
      "=================================================================\n",
      "Total params: 18,563,367\n",
      "Trainable params: 18,563,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_dec = tf.random.uniform((BATCH_SIZE, sequence_len+7))\n",
    "\n",
    "sample_enc = tf.keras.preprocessing.sequence.pad_sequences(sample_enc, padding='post', maxlen=MAX_LEN)\n",
    "sample_dec = tf.keras.preprocessing.sequence.pad_sequences(sample_dec, padding='post', maxlen=MAX_LEN)\n",
    "\n",
    "enc_mask, combined_mask, dec_enc_mask = \\\n",
    "    generate_masks(sample_enc, sample_dec)\n",
    "\n",
    "sample_logits, enc_attns, dec_attns, dec_enc_attns = transformer(\n",
    "        sample_enc, sample_dec, enc_mask, combined_mask, dec_enc_mask )\n",
    "\n",
    "print ('enc_mask:', enc_mask.shape) # .shape\n",
    "print ('dec_enc_mask:', dec_enc_mask.shape) # .shape\n",
    "print ('dec_mask:', combined_mask.shape) # .shape\n",
    "print ('enc_attns:', enc_attns[0].shape) # .shape\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('dec_attns:', dec_attns[0].shape) # .shape\n",
    "print ('dec_enc_attns:', dec_enc_attns[0].shape) # .shape\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87e97564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_answer(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    '''\n",
    "    translate와 동일\n",
    "    '''\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                            [tokens], maxlen=MAX_LEN, padding='post' )\n",
    "    ids = []\n",
    "    # output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)   \n",
    "    output = tf.expand_dims([tgt_tokenizer.word_index[\"<bos>\"]], 0)   \n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "                                        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        # if tgt_tokenizer.eos_id() == predicted_id:\n",
    "        if tgt_tokenizer.word_index[\"<eos>\"] == predicted_id:\n",
    "            # result = tgt_tokenizer.decode_ids(ids)  \n",
    "            result = tgt_tokenizer.sequences_to_texts([ids])  \n",
    "            return result[0]\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    # result = tgt_tokenizer.decode_ids(ids)\n",
    "    result = tgt_tokenizer.sequences_to_texts([ids])  \n",
    "    \n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acf712fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_count)\n",
    "run_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56871f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(enc_train))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3801b6e0",
   "metadata": {},
   "source": [
    "# 이전에 사용한 tf API 자료 입력\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56a3baa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbbc10e06b1448fb34f20b3aeb79062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967add76b8094995b119065a082c5edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec5a14a25364118b0e77cea70337195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2038392e8442a0b5985f8abd51509a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ffe1786e7048aba487d47c43d4310e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683229702cc3490cb79f19cb9a12695e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  지하철 만 타 면 생각나                         \n",
      "tgt:  양 을 세 어 보 지 마세요 .\n",
      "src:  너 누구 ...                           \n",
      "tgt:  저 는 고민 이 네요 .\n",
      "src:  6 개월 간 인의 짧 은 사랑 .                      \n",
      "tgt:  그 사람 을 위하 는 거 라고 생각 하 는 말 을 한 역할 을 합니다 .\n",
      "src:  완전 재미 없 거든                          \n",
      "tgt:  나중 에 도움 이 될 거 예요 .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccb616eab884788ae1d85b51dc2daf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e070772b744e4297741ef900b6fc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1674c0ec3ee410ca44a13810def2868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3dc95f88d34ed0ad78a8ec4f96a06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5697180315429b90717e9db2f81eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c154010f4dd7444d93ca18d641447025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  지하철 만 타 면 생각나                         \n",
      "tgt:  아무런 생각 도 안 나 는 날 이 올 거 예요 .\n",
      "src:  너 누구 ...                           \n",
      "tgt:  저 는 마음 이 이어주 는 딸기 잼 에게 물어봐 주 세요 .\n",
      "src:  6 개월 간 인의 짧 은 사랑 .                      \n",
      "tgt:  그만큼 애틋 한두 사랑 이 였 겠 어요 .\n",
      "src:  완전 재미 없 거든                          \n",
      "tgt:  웃 을 때 가 있 을 거 예요 .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8fd5fa67714056887fdb00c06042a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8759b780302446281ff2abe144cc840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696cf785ca7549ffb4a3c3736b78e4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb3076a9db34d60941817f3c8259844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef63b231b97b43778632be758ae84eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb18bc59c9df47f09ca7a85633768dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  지하철 만 타 면 생각나                         \n",
      "tgt:  아무런 생각 도 안 나 는 날 이 올 거 예요 .\n",
      "src:  너 누구 ...                           \n",
      "tgt:  저 는 마음 을 이어주 는 위 로 봇 입니다 .\n",
      "src:  6 개월 간 인의 짧 은 사랑 .                      \n",
      "tgt:  그만큼 애틋 한 사랑 이 였 겠 어요 .\n",
      "src:  완전 재미 없 거든                          \n",
      "tgt:  웃 을 때 까지 이야기 해 보 죠\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b348891c90f7410aa9fe22fce0e777d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a699e09c81ff42c0917fe7e39c1f2f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b201bdad55f48b3a18644561f436ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f91e2ed7ee84247a0e6a55c4d9f8485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5422e73b93a4890828dbed55fa5120e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f2670f154a4089b6dafaee74a5a570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  지하철 만 타 면 생각나                         \n",
      "tgt:  아무런 생각 도 안 나 는 날 이 올 거 예요 .\n",
      "src:  너 누구 ...                           \n",
      "tgt:  저 는 마음 을 이어주 는 위 로 봇 입니다 .\n",
      "src:  6 개월 간 인의 짧 은 사랑 .                      \n",
      "tgt:  그만큼 애틋 한 사랑 이 였 겠 어요 .\n",
      "src:  완전 재미 없 거든                          \n",
      "tgt:  웃 을 때 까지 이야기 해 주 죠\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, len(enc_train), BATCH_SIZE)) # \n",
    "    random.shuffle(idx_list)\n",
    "    \n",
    "    t = tqdm(idx_list)\n",
    "    t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        if len(enc_train[idx:idx+BATCH_SIZE]) < BATCH_SIZE: continue\n",
    "        batch_loss, _, _, _ = train_step(enc_train[idx:idx+BATCH_SIZE], ##\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                transformer,\n",
    "                                optimizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        # print(batch_loss.numpy())\n",
    "        # t.set_postfix_str('Loss %.4f' % float(batch_loss.numpy() ) )\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    if (epoch+1) % 5 == 0: pass\n",
    "    elif epoch+1 == EPOCHS: pass\n",
    "    else: continue\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, len(enc_val), BATCH_SIZE)) # \n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "    \n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    transformer,)\n",
    "    \n",
    "        test_loss += test_batch_loss\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))\n",
    "    \n",
    "    for example in enc_val[:16:4]:\n",
    "        print(\"src: \", ko_tokenizer.sequences_to_texts([example])[0])\n",
    "        tgt = gen_answer(example, transformer, ko_tokenizer, ko_tokenizer)\n",
    "        print(\"tgt: \", tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94ca4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    \n",
    "    if isinstance(src_sentence[0], np.int32):\n",
    "        src_tokens = src_sentence\n",
    "        tgt_tokens = [t for t in tgt_sentence if t!=3 and t!=4] # remove <bos>, <eos>\n",
    "        src_sentence = src_tokenizer.sequences_to_texts([src_tokens])[0]\n",
    "        tgt_sentence = tgt_tokenizer.sequences_to_texts([tgt_tokens])[0]\n",
    "    else:\n",
    "        src_tokens = src_tokenizer.texts_to_sequences([src_sentence])[0]\n",
    "        tgt_tokens = tgt_tokenizer.texts_to_sequences([tgt_sentence])[0]\n",
    "\n",
    "    if (len(src_tokens) > MAX_LEN): return None\n",
    "    if (len(tgt_tokens) > MAX_LEN): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = gen_answer(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54edb63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx],\n",
    "                                 src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa9cc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21663cc7",
   "metadata": {},
   "source": [
    "enc_val[test_idx][0], isinstance(enc_val[test_idx][0], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f46a572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  지하철 만 타 면 생각나                         \n",
      "Model Prediction:  ['아무런', '생각', '도', '안', '나', '는', '날', '이', '올', '거', '예요', '.']\n",
      "Real:  ['아무런', '생각', '도', '안', '나', 'ㄴ다는', '날', '이', '올', '거', '예요', '.']\n",
      "Score: 0.734889\n",
      "\n",
      "Source Sentence:  교회 에 좋 아 하 는 여자 애 가 생겼 어 .                  \n",
      "Model Prediction:  ['토요일', '이', '기다려', '지', '겠', '네요', '.']\n",
      "Real:  ['일요일', '이', '기다려', '지', '겠', '네요', '.']\n",
      "Score: 0.809107\n",
      "\n",
      "Source Sentence:  이별 통보 후 일 주일 이 네                       \n",
      "Model Prediction:  ['마음', '이', '후련', '하', '지만은', '않', '겠', '네요', '.']\n",
      "Real:  ['마음', '이', '후련', '하', '지만은', '않', '겠', '네요', '.']\n",
      "Score: 1.000000\n",
      "\n",
      "Source Sentence:  나 한테 뒷모습 만 보여 줬 어                       \n",
      "Model Prediction:  ['이제', '앞모습', '을', '볼', '차례', '네요', '.']\n",
      "Real:  ['이제', '앞모습', '을', '볼', '차례', '네요', '.']\n",
      "Score: 1.000000\n",
      "\n",
      "Source Sentence:  내 주제 을 모르 고 덤빈 건가                       \n",
      "Model Prediction:  ['그건', '아닐', '거', '예요', '.']\n",
      "Real:  ['그건', '아닐', '거', '예요', '.']\n",
      "Score: 1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_idx = 0\n",
    "for test_idx in range(0,100,20):\n",
    "    eval_bleu_single(transformer, \n",
    "                     enc_val[test_idx], \n",
    "                     dec_val[test_idx], \n",
    "                     ko_tokenizer, \n",
    "                     ko_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff5cbd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2e55a9d5e44816a09909fb1d19fe79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 1013\n",
      "Total Score: 0.681718300084504\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, enc_val, dec_val, ko_tokenizer, ko_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "939026db",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmpls= ['어디 있었어?',\n",
    "\"완전 대박이네\",\n",
    "\"1지망 학교 떨어졌는데 3박 4일 놀러가도 될까?\",\n",
    "\"오늘 나 완전 기분 꿀꿀한데 저녁 뭐 먹을까\",\n",
    "\"오늘나완전기분꿀꿀한데저녁뭐먹을까\",\n",
    "\"오늘나완전기분꿀꿀\",\n",
    "\"오늘 나랑 밥 먹을래?\",\n",
    "\"오늘 나랑 lunch 먹을래?\",\n",
    "\"오늘 나랑 3 먹을래?\",\n",
    "\"오늘 나랑 PPL 갈래?\",\n",
    "\"오늘 너 좀 예쁘네\",\n",
    "\"오늴 니 쫌 이쁘네\",\n",
    "\"썸\",]\n",
    "\n",
    "padded_xmpls = padded_seqs(xmpls, ko_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5eadd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  어디 있었어?\n",
      "A:  현실 있 었 어요 .\n",
      "\n",
      "Q:  완전 대박이네\n",
      "A:  감동 적 인 거리 두 기억 이 라도 이 에요 .\n",
      "\n",
      "Q:  1지망 학교 떨어졌는데 3박 4일 놀러가도 될까?\n",
      "A:  서로 의 마음 이 중요 하 네요 .\n",
      "\n",
      "Q:  오늘 나 완전 기분 꿀꿀한데 저녁 뭐 먹을까\n",
      "A:  큰 문제 는 남 을 수 있 겠 어요 .\n",
      "\n",
      "Q:  오늘나완전기분꿀꿀한데저녁뭐먹을까\n",
      "A:  마음 을 접 는 게 더 힘들 었 을 텐데 맘고생 많 았 어요 .\n",
      "\n",
      "Q:  오늘나완전기분꿀꿀\n",
      "A:  남 에게 피해 주 지 않 는 게 좋 겠 어요 .\n",
      "\n",
      "Q:  오늘 나랑 밥 먹을래?\n",
      "A:  즐거운 데이트 서운 시키 겠 어요 .\n",
      "\n",
      "Q:  오늘 나랑 lunch 먹을래?\n",
      "A:  버릴 줄 친구 라면 좋 지 않 아요 .\n",
      "\n",
      "Q:  오늘 나랑 3 먹을래?\n",
      "A:  최선 의 네요 .\n",
      "\n",
      "Q:  오늘 나랑 PPL 갈래?\n",
      "A:  천천히 당사자 만 알 겠 어요 .\n",
      "\n",
      "Q:  오늘 너 좀 예쁘네\n",
      "A:  그런 날 몰랐 는 분 인가 봐요 .\n",
      "\n",
      "Q:  오늴 니 쫌 이쁘네\n",
      "A:  기다리 는 텐데 .\n",
      "\n",
      "Q:  썸\n",
      "A:  연락 하 얼마나 연락 오 는지 입증 하 얼마나 연락 해 보 세요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(padded_xmpls):\n",
    "    print(\"Q: \", xmpls[i])\n",
    "    print(\"A: \", gen_answer(seq, transformer, ko_tokenizer, ko_tokenizer))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9b2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
